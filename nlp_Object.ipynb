{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "organic-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "automated-steam",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cubic-temple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18446744073709551615\n",
      "18446744073709551615\n",
      "18446744073709551615\n",
      "18446744073709551615\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Hello World!!\")\n",
    "for token in doc:\n",
    "    print(token.rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "connected-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lesser-dover",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                                                                                                                                                                                                                                                                                                                                feedback\\n                                                                                                                                                                                                                                Thank you for today's session. It was amazing and I look forward to starting the journey of Data Science\\n                                                                                                                                                                                         The session was engaging its getting abit tougher as we continue but your explanations really easen things out ,otherwise i enjoyed the session\\n                                                                                                                                                                                                                                                                    the session was very good, already waiting for the data science part\\n                                                                                                                                                                                                                        the class was good and I have enjoyed it still having some small challenge but hope it will be fine  and thank u\\n                                                                                                                                                                                     Thank you for the class and the effort you put in. I am quite interested in the coming lesson which am sure will touch on a few data science basics\\nThe session was interactive and fun. I enjoyed learning the conda prompt commands, makes me feel geekishly excited to see it work. I saw what you said about the conda GUI using up a lot of RAM.\\\\r\\\\nI had a challenge accessing this form. Is it possible to allocate a channel for this form?\\\\r\\\\nLooking foward to today's lesson.\\\\r\\\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/feedbackexport.csv')\n",
    "feed = df.to_string(index = False)\n",
    "feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "urban-continent",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13735693502083210583 0 0\n",
      "13110060611322374290 0 0\n",
      "13735693502083210583 0 0\n",
      "16072095006890171862 0 0\n",
      "4088098365541558500 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "3155823383004454315 0 0\n",
      "13110060611322374290 0 0\n",
      "12646065887601541794 0 0\n",
      "12204527652707022206 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "4088098365541558500 0 0\n",
      "101 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "4370460163704169311 0 0\n",
      "13110060611322374290 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "4370460163704169311 0 0\n",
      "10887629174180191697 0 0\n",
      "16072095006890171862 0 0\n",
      "13735693502083210583 0 0\n",
      "18200729499220207786 0 0\n",
      "13110060611322374290 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "4370460163704169311 0 0\n",
      "4370460163704169311 0 0\n",
      "13110060611322374290 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "4088098365541558500 0 0\n",
      "2593208677638477497 0 0\n",
      "13110060611322374290 0 0\n",
      "11123243248953317070 0 0\n",
      "13110060611322374290 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "13735693502083210583 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "2593208677638477497 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "4088098365541558500 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "13735693502083210583 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "4088098365541558500 0 0\n",
      "101 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "4370460163704169311 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "4370460163704169311 0 0\n",
      "13110060611322374290 0 0\n",
      "4370460163704169311 0 0\n",
      "13110060611322374290 0 0\n",
      "8532415787641010193 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "11123243248953317070 0 0\n",
      "13735693502083210583 0 0\n",
      "16072095006890171862 0 0\n",
      "4088098365541558500 0 0\n",
      "4088098365541558500 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "4088098365541558500 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "4088098365541558500 0 0\n",
      "4088098365541558500 0 0\n",
      "4370460163704169311 0 0\n",
      "12646065887601541794 0 0\n",
      "101 0 0\n",
      "4370460163704169311 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "4370460163704169311 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "4370460163704169311 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "4370460163704169311 0 0\n",
      "11123243248953317070 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "962983613142996970 0 0\n",
      "18200729499220207786 0 0\n",
      "13110060611322374290 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "4088098365541558500 0 0\n",
      "4088098365541558500 0 0\n",
      "12646065887601541794 0 0\n",
      "101 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "2593208677638477497 0 0\n",
      "13110060611322374290 0 0\n",
      "4370460163704169311 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "4370460163704169311 0 0\n",
      "4088098365541558500 0 0\n",
      "4370460163704169311 0 0\n",
      "13110060611322374290 0 0\n",
      "12646065887601541794 0 0\n",
      "101 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "3552942401566437853 0 0\n",
      "13110060611322374290 0 0\n",
      "4370460163704169311 0 0\n",
      "11123243248953317070 0 0\n",
      "4088098365541558500 0 0\n",
      "4370460163704169311 0 0\n",
      "14216178875182100022 0 0\n",
      "4088098365541558500 0 0\n",
      "11123243248953317070 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "13110060611322374290 0 0\n",
      "12646065887601541794 0 0\n",
      "12204527652707022206 0 0\n",
      "4370460163704169311 0 0\n",
      "13110060611322374290 0 0\n",
      "4370460163704169311 0 0\n",
      "13110060611322374290 0 0\n",
      "11123243248953317070 0 0\n",
      "13110060611322374290 0 0\n",
      "4088098365541558500 0 0\n",
      "13110060611322374290 0 0\n",
      "12651538343924498880 0 0\n",
      "13110060611322374290 0 0\n",
      "4370460163704169311 0 0\n",
      "13110060611322374290 0 0\n",
      "3155823383004454315 0 0\n",
      "15870074177902712730 0 0\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(feed)\n",
    "for token in doc:\n",
    "    print(token.shape, token.tag, token.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "phantom-fiction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lesson.\\r\\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(feed)\n",
    "doc[182]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "practical-upper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                feedback\n",
       "                                                                                                                                                                                                                                "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span = doc[0:3]\n",
    "span"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-basin",
   "metadata": {},
   "source": [
    "## LEXICAL ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "graphic-viewer",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182]\n",
      "text ['                                                                                                                                                                                                                                                                                                                                ', 'feedback', '\\n                                                                                                                                                                                                                                ', 'Thank', 'you', 'for', 'today', \"'s\", 'session', '.', 'It', 'was', 'amazing', 'and', 'I', 'look', 'forward', 'to', 'starting', 'the', 'journey', 'of', 'Data', 'Science', '\\n                                                                                                                                                                                         ', 'The', 'session', 'was', 'engaging', 'its', 'getting', 'abit', 'tougher', 'as', 'we', 'continue', 'but', 'your', 'explanations', 'really', 'easen', 'things', 'out', ',', 'otherwise', 'i', 'enjoyed', 'the', 'session', '\\n                                                                                                                                                                                                                                                                    ', 'the', 'session', 'was', 'very', 'good', ',', 'already', 'waiting', 'for', 'the', 'data', 'science', 'part', '\\n                                                                                                                                                                                                                        ', 'the', 'class', 'was', 'good', 'and', 'I', 'have', 'enjoyed', 'it', 'still', 'having', 'some', 'small', 'challenge', 'but', 'hope', 'it', 'will', 'be', 'fine', ' ', 'and', 'thank', 'u', '\\n                                                                                                                                                                                     ', 'Thank', 'you', 'for', 'the', 'class', 'and', 'the', 'effort', 'you', 'put', 'in', '.', 'I', 'am', 'quite', 'interested', 'in', 'the', 'coming', 'lesson', 'which', 'am', 'sure', 'will', 'touch', 'on', 'a', 'few', 'data', 'science', 'basics', '\\n', 'The', 'session', 'was', 'interactive', 'and', 'fun', '.', 'I', 'enjoyed', 'learning', 'the', 'conda', 'prompt', 'commands', ',', 'makes', 'me', 'feel', 'geekishly', 'excited', 'to', 'see', 'it', 'work', '.', 'I', 'saw', 'what', 'you', 'said', 'about', 'the', 'conda', 'GUI', 'using', 'up', 'a', 'lot', 'of', 'RAM.\\\\r\\\\nI', 'had', 'a', 'challenge', 'accessing', 'this', 'form', '.', 'Is', 'it', 'possible', 'to', 'allocate', 'a', 'channel', 'for', 'this', 'form?\\\\r\\\\nLooking', 'foward', 'to', 'today', \"'s\", 'lesson.\\\\r\\\\n']\n",
      "Is Alpha [False, True, False, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, True, True, False, False]\n",
      "Ïs punct [False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Is oov [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Is like Num [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(feed)\n",
    "print(\"index\", [token.i for token in doc])\n",
    "print(\"text\", [token.text for token in doc])\n",
    "print(\"Is Alpha\", [token.is_alpha for token in doc])\n",
    "print(\"Ïs punct\", [token.is_punct for token in doc])\n",
    "print(\"Is oov\", [token.is_oov for token in doc])\n",
    "print(\"Is like Num\", [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "subsequent-sending",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "print([token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "swiss-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(feed)\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i + 1]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)\n",
    "        elif next_token.text != \"%\":\n",
    "            print(\"None\")\n",
    "    #else:\n",
    "     #   print(\"No numbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pressing-virgin",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                \n",
      "feedback\n",
      "\n",
      "                                                                                                                                                                                                                                \n",
      "Thank\n",
      "you\n",
      "for\n",
      "today\n",
      "'s\n",
      "session\n",
      ".\n",
      "It\n",
      "was\n",
      "amazing\n",
      "and\n",
      "I\n",
      "look\n",
      "forward\n",
      "to\n",
      "starting\n",
      "the\n",
      "journey\n",
      "of\n",
      "Data\n",
      "Science\n",
      "\n",
      "                                                                                                                                                                                         \n",
      "The\n",
      "session\n",
      "was\n",
      "engaging\n",
      "its\n",
      "getting\n",
      "abit\n",
      "tougher\n",
      "as\n",
      "we\n",
      "continue\n",
      "but\n",
      "your\n",
      "explanations\n",
      "really\n",
      "easen\n",
      "things\n",
      "out\n",
      ",\n",
      "otherwise\n",
      "i\n",
      "enjoyed\n",
      "the\n",
      "session\n",
      "\n",
      "                                                                                                                                                                                                                                                                    \n",
      "the\n",
      "session\n",
      "was\n",
      "very\n",
      "good\n",
      ",\n",
      "already\n",
      "waiting\n",
      "for\n",
      "the\n",
      "data\n",
      "science\n",
      "part\n",
      "\n",
      "                                                                                                                                                                                                                        \n",
      "the\n",
      "class\n",
      "was\n",
      "good\n",
      "and\n",
      "I\n",
      "have\n",
      "enjoyed\n",
      "it\n",
      "still\n",
      "having\n",
      "some\n",
      "small\n",
      "challenge\n",
      "but\n",
      "hope\n",
      "it\n",
      "will\n",
      "be\n",
      "fine\n",
      " \n",
      "and\n",
      "thank\n",
      "u\n",
      "\n",
      "                                                                                                                                                                                     \n",
      "Thank\n",
      "you\n",
      "for\n",
      "the\n",
      "class\n",
      "and\n",
      "the\n",
      "effort\n",
      "you\n",
      "put\n",
      "in\n",
      ".\n",
      "I\n",
      "am\n",
      "quite\n",
      "interested\n",
      "in\n",
      "the\n",
      "coming\n",
      "lesson\n",
      "which\n",
      "am\n",
      "sure\n",
      "will\n",
      "touch\n",
      "on\n",
      "a\n",
      "few\n",
      "data\n",
      "science\n",
      "basics\n",
      "\n",
      "\n",
      "The\n",
      "session\n",
      "was\n",
      "interactive\n",
      "and\n",
      "fun\n",
      ".\n",
      "I\n",
      "enjoyed\n",
      "learning\n",
      "the\n",
      "conda\n",
      "prompt\n",
      "commands\n",
      ",\n",
      "makes\n",
      "me\n",
      "feel\n",
      "geekishly\n",
      "excited\n",
      "to\n",
      "see\n",
      "it\n",
      "work\n",
      ".\n",
      "I\n",
      "saw\n",
      "what\n",
      "you\n",
      "said\n",
      "about\n",
      "the\n",
      "conda\n",
      "GUI\n",
      "using\n",
      "up\n",
      "a\n",
      "lot\n",
      "of\n",
      "RAM.\\r\\nI\n",
      "had\n",
      "a\n",
      "challenge\n",
      "accessing\n",
      "this\n",
      "form\n",
      ".\n",
      "Is\n",
      "it\n",
      "possible\n",
      "to\n",
      "allocate\n",
      "a\n",
      "channel\n",
      "for\n",
      "this\n",
      "form?\\r\\nLooking\n",
      "foward\n",
      "to\n",
      "today\n",
      "'s\n",
      "lesson.\\r\\n\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(feed)\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-durham",
   "metadata": {},
   "source": [
    "## STATISTICAL MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "raising-attempt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                 SPACE\n",
      "feedback NOUN\n",
      "\n",
      "                                                                                                                                                                                                                                 SPACE\n",
      "Thank VERB\n",
      "you PRON\n",
      "for ADP\n",
      "today NOUN\n",
      "'s PART\n",
      "session NOUN\n",
      ". PUNCT\n",
      "It PRON\n",
      "was AUX\n",
      "amazing ADJ\n",
      "and CCONJ\n",
      "I PRON\n",
      "look VERB\n",
      "forward ADV\n",
      "to ADP\n",
      "starting VERB\n",
      "the DET\n",
      "journey NOUN\n",
      "of ADP\n",
      "Data PROPN\n",
      "Science PROPN\n",
      "\n",
      "                                                                                                                                                                                          SPACE\n",
      "The DET\n",
      "session NOUN\n",
      "was AUX\n",
      "engaging VERB\n",
      "its PRON\n",
      "getting VERB\n",
      "abit ADV\n",
      "tougher ADJ\n",
      "as ADP\n",
      "we PRON\n",
      "continue VERB\n",
      "but CCONJ\n",
      "your PRON\n",
      "explanations NOUN\n",
      "really ADV\n",
      "easen VERB\n",
      "things NOUN\n",
      "out ADP\n",
      ", PUNCT\n",
      "otherwise ADV\n",
      "i PRON\n",
      "enjoyed VERB\n",
      "the DET\n",
      "session NOUN\n",
      "\n",
      "                                                                                                                                                                                                                                                                     SPACE\n",
      "the DET\n",
      "session NOUN\n",
      "was AUX\n",
      "very ADV\n",
      "good ADJ\n",
      ", PUNCT\n",
      "already ADV\n",
      "waiting VERB\n",
      "for ADP\n",
      "the DET\n",
      "data NOUN\n",
      "science NOUN\n",
      "part NOUN\n",
      "\n",
      "                                                                                                                                                                                                                         SPACE\n",
      "the DET\n",
      "class NOUN\n",
      "was VERB\n",
      "good ADJ\n",
      "and CCONJ\n",
      "I PRON\n",
      "have AUX\n",
      "enjoyed VERB\n",
      "it PRON\n",
      "still ADV\n",
      "having VERB\n",
      "some DET\n",
      "small ADJ\n",
      "challenge NOUN\n",
      "but CCONJ\n",
      "hope VERB\n",
      "it PRON\n",
      "will AUX\n",
      "be VERB\n",
      "fine ADJ\n",
      "  SPACE\n",
      "and CCONJ\n",
      "thank VERB\n",
      "u NOUN\n",
      "\n",
      "                                                                                                                                                                                      SPACE\n",
      "Thank VERB\n",
      "you PRON\n",
      "for ADP\n",
      "the DET\n",
      "class NOUN\n",
      "and CCONJ\n",
      "the DET\n",
      "effort NOUN\n",
      "you PRON\n",
      "put VERB\n",
      "in ADP\n",
      ". PUNCT\n",
      "I PRON\n",
      "am AUX\n",
      "quite ADV\n",
      "interested ADJ\n",
      "in ADP\n",
      "the DET\n",
      "coming VERB\n",
      "lesson NOUN\n",
      "which DET\n",
      "am VERB\n",
      "sure ADJ\n",
      "will AUX\n",
      "touch VERB\n",
      "on ADP\n",
      "a DET\n",
      "few ADJ\n",
      "data NOUN\n",
      "science NOUN\n",
      "basics NOUN\n",
      "\n",
      " SPACE\n",
      "The DET\n",
      "session NOUN\n",
      "was AUX\n",
      "interactive ADJ\n",
      "and CCONJ\n",
      "fun ADJ\n",
      ". PUNCT\n",
      "I PRON\n",
      "enjoyed AUX\n",
      "learning VERB\n",
      "the DET\n",
      "conda NOUN\n",
      "prompt NOUN\n",
      "commands NOUN\n",
      ", PUNCT\n",
      "makes VERB\n",
      "me PRON\n",
      "feel VERB\n",
      "geekishly ADV\n",
      "excited ADJ\n",
      "to PART\n",
      "see VERB\n",
      "it PRON\n",
      "work VERB\n",
      ". PUNCT\n",
      "I PRON\n",
      "saw VERB\n",
      "what PRON\n",
      "you PRON\n",
      "said VERB\n",
      "about ADP\n",
      "the DET\n",
      "conda NOUN\n",
      "GUI PROPN\n",
      "using VERB\n",
      "up ADP\n",
      "a DET\n",
      "lot NOUN\n",
      "of ADP\n",
      "RAM.\\r\\nI PROPN\n",
      "had VERB\n",
      "a DET\n",
      "challenge NOUN\n",
      "accessing VERB\n",
      "this DET\n",
      "form NOUN\n",
      ". PUNCT\n",
      "Is AUX\n",
      "it PRON\n",
      "possible ADJ\n",
      "to PART\n",
      "allocate VERB\n",
      "a DET\n",
      "channel NOUN\n",
      "for ADP\n",
      "this DET\n",
      "form?\\r\\nLooking NOUN\n",
      "foward ADV\n",
      "to ADP\n",
      "today NOUN\n",
      "'s PART\n",
      "lesson.\\r\\n X\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(feed)\n",
    "#print POS tag\n",
    "for token in doc:\n",
    "    print( token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-syndication",
   "metadata": {},
   "source": [
    "### Predicting Syntactic Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "portuguese-native",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj sells\n",
      "sells VERB ROOT sells\n",
      "sea NOUN compound shells\n",
      "shells NOUN dobj sells\n",
      "at ADP prep sells\n",
      "the DET det shore\n",
      "sea NOUN compound shore\n",
      "shore NOUN pobj at\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"She sells sea shells at the sea shore\")\n",
    "# print text, POS, dependency, parent token the word is attached to\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-mongolia",
   "metadata": {},
   "source": [
    "### Predicting Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "revolutionary-florist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today DATE\n",
      "Data Science\n",
      "                                                                                                                                                                                          PRODUCT\n",
      "RAM.\\r\\nI ORG\n",
      "form?\\r\\nLooking ORG\n",
      "today DATE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(feed)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "confident-furniture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "first ORDINAL\n",
      "America GPE\n",
      "Shillings CARDINAL\n",
      "S&P500 PRODUCT\n",
      "NYSE ORG\n",
      "2020 DATE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is the first company in America to reach a Shillings trading at the S&P500 and NYSE in 2020\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aquatic-contract",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kenya GPE\n",
      "Rwanda GPE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Moringa school offers the best data science and software development courses in Kenya and Rwanda\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "quality-soldier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monetary values, including unit'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"MONEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "recorded-nothing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'auxiliary'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"AUX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "unlikely-resolution",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                SPACE     nsubj     \n",
      "feedback    NOUN      ROOT      \n",
      "\n",
      "                                                                                                                                                                                                                                SPACE     nsubj     \n",
      "Thank       VERB      ROOT      \n",
      "you         PRON      dobj      \n",
      "for         ADP       prep      \n",
      "today       NOUN      poss      \n",
      "'s          PART      case      \n",
      "session     NOUN      pobj      \n",
      ".           PUNCT     punct     \n",
      "It          PRON      nsubj     \n",
      "was         AUX       ROOT      \n",
      "amazing     ADJ       acomp     \n",
      "and         CCONJ     cc        \n",
      "I           PRON      nsubj     \n",
      "look        VERB      conj      \n",
      "forward     ADV       advmod    \n",
      "to          ADP       prep      \n",
      "starting    VERB      pcomp     \n",
      "the         DET       det       \n",
      "journey     NOUN      dobj      \n",
      "of          ADP       prep      \n",
      "Data        PROPN     compound  \n",
      "Science     PROPN     compound  \n",
      "\n",
      "                                                                                                                                                                                         SPACE     pobj      \n",
      "The         DET       det       \n",
      "session     NOUN      nsubj     \n",
      "was         AUX       aux       \n",
      "engaging    VERB      ROOT      \n",
      "its         PRON      poss      \n",
      "getting     VERB      dobj      \n",
      "abit        ADV       advmod    \n",
      "tougher     ADJ       acomp     \n",
      "as          ADP       mark      \n",
      "we          PRON      nsubj     \n",
      "continue    VERB      advcl     \n",
      "but         CCONJ     cc        \n",
      "your        PRON      poss      \n",
      "explanationsNOUN      nsubj     \n",
      "really      ADV       advmod    \n",
      "easen       VERB      conj      \n",
      "things      NOUN      dobj      \n",
      "out         ADP       prt       \n",
      ",           PUNCT     punct     \n",
      "otherwise   ADV       advmod    \n",
      "i           PRON      nsubj     \n",
      "enjoyed     VERB      conj      \n",
      "the         DET       det       \n",
      "session     NOUN      dobj      \n",
      "\n",
      "                                                                                                                                                                                                                                                                    SPACE     dep       \n",
      "the         DET       det       \n",
      "session     NOUN      nsubj     \n",
      "was         AUX       ccomp     \n",
      "very        ADV       advmod    \n",
      "good        ADJ       acomp     \n",
      ",           PUNCT     punct     \n",
      "already     ADV       advmod    \n",
      "waiting     VERB      advcl     \n",
      "for         ADP       prep      \n",
      "the         DET       det       \n",
      "data        NOUN      compound  \n",
      "science     NOUN      compound  \n",
      "part        NOUN      pobj      \n",
      "\n",
      "                                                                                                                                                                                                                        SPACE     appos     \n",
      "the         DET       det       \n",
      "class       NOUN      nsubj     \n",
      "was         VERB      conj      \n",
      "good        ADJ       acomp     \n",
      "and         CCONJ     cc        \n",
      "I           PRON      nsubj     \n",
      "have        AUX       aux       \n",
      "enjoyed     VERB      conj      \n",
      "it          PRON      nsubj     \n",
      "still       ADV       advmod    \n",
      "having      VERB      ccomp     \n",
      "some        DET       det       \n",
      "small       ADJ       amod      \n",
      "challenge   NOUN      dobj      \n",
      "but         CCONJ     cc        \n",
      "hope        VERB      conj      \n",
      "it          PRON      nsubj     \n",
      "will        AUX       aux       \n",
      "be          VERB      ccomp     \n",
      "fine        ADJ       acomp     \n",
      "            SPACE     dep       \n",
      "and         CCONJ     cc        \n",
      "thank       VERB      conj      \n",
      "u           NOUN      dobj      \n",
      "\n",
      "                                                                                                                                                                                     SPACE     npadvmod  \n",
      "Thank       VERB      conj      \n",
      "you         PRON      dobj      \n",
      "for         ADP       prep      \n",
      "the         DET       det       \n",
      "class       NOUN      pobj      \n",
      "and         CCONJ     cc        \n",
      "the         DET       det       \n",
      "effort      NOUN      conj      \n",
      "you         PRON      nsubj     \n",
      "put         VERB      relcl     \n",
      "in          ADP       prt       \n",
      ".           PUNCT     punct     \n",
      "I           PRON      nsubj     \n",
      "am          AUX       ROOT      \n",
      "quite       ADV       advmod    \n",
      "interested  ADJ       acomp     \n",
      "in          ADP       prep      \n",
      "the         DET       det       \n",
      "coming      VERB      amod      \n",
      "lesson      NOUN      pobj      \n",
      "which       DET       nsubj     \n",
      "am          VERB      relcl     \n",
      "sure        ADJ       acomp     \n",
      "will        AUX       aux       \n",
      "touch       VERB      xcomp     \n",
      "on          ADP       prep      \n",
      "a           DET       quantmod  \n",
      "few         ADJ       amod      \n",
      "data        NOUN      compound  \n",
      "science     NOUN      compound  \n",
      "basics      NOUN      pobj      \n",
      "\n",
      "           SPACE     punct     \n",
      "The         DET       det       \n",
      "session     NOUN      nsubj     \n",
      "was         AUX       ROOT      \n",
      "interactive ADJ       acomp     \n",
      "and         CCONJ     cc        \n",
      "fun         ADJ       conj      \n",
      ".           PUNCT     punct     \n",
      "I           PRON      nsubj     \n",
      "enjoyed     AUX       aux       \n",
      "learning    VERB      ROOT      \n",
      "the         DET       det       \n",
      "conda       NOUN      compound  \n",
      "prompt      NOUN      compound  \n",
      "commands    NOUN      dobj      \n",
      ",           PUNCT     punct     \n",
      "makes       VERB      conj      \n",
      "me          PRON      nsubj     \n",
      "feel        VERB      ccomp     \n",
      "geekishly   ADV       advmod    \n",
      "excited     ADJ       acomp     \n",
      "to          PART      aux       \n",
      "see         VERB      xcomp     \n",
      "it          PRON      nsubj     \n",
      "work        VERB      ccomp     \n",
      ".           PUNCT     punct     \n",
      "I           PRON      nsubj     \n",
      "saw         VERB      ROOT      \n",
      "what        PRON      dobj      \n",
      "you         PRON      nsubj     \n",
      "said        VERB      ccomp     \n",
      "about       ADP       prep      \n",
      "the         DET       det       \n",
      "conda       NOUN      compound  \n",
      "GUI         PROPN     pobj      \n",
      "using       VERB      pcomp     \n",
      "up          ADP       prt       \n",
      "a           DET       det       \n",
      "lot         NOUN      dobj      \n",
      "of          ADP       prep      \n",
      "RAM.\\r\\nI   PROPN     pobj      \n",
      "had         VERB      ccomp     \n",
      "a           DET       det       \n",
      "challenge   NOUN      dobj      \n",
      "accessing   VERB      acl       \n",
      "this        DET       det       \n",
      "form        NOUN      dobj      \n",
      ".           PUNCT     punct     \n",
      "Is          AUX       ROOT      \n",
      "it          PRON      nsubj     \n",
      "possible    ADJ       acomp     \n",
      "to          PART      aux       \n",
      "allocate    VERB      xcomp     \n",
      "a           DET       det       \n",
      "channel     NOUN      dobj      \n",
      "for         ADP       prep      \n",
      "this        DET       det       \n",
      "form?\\r\\nLookingNOUN      pobj      \n",
      "foward      ADV       advmod    \n",
      "to          ADP       prep      \n",
      "today       NOUN      poss      \n",
      "'s          PART      case      \n",
      "lesson.\\r\\n X         pobj      \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(feed)\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "familiar-master",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple <built-in method get_extension of type object at 0x00007FFD51621D00> ORG\n",
      "first <built-in method get_extension of type object at 0x00007FFD51621D00> ORDINAL\n",
      "U.S. <built-in method get_extension of type object at 0x00007FFD51621D00> GPE\n",
      "$1 trillion <built-in method get_extension of type object at 0x00007FFD51621D00> MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"it’s official: Apple is the first U.S. public company to reach a $1 trillion market value\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.get_extension, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "acute-vampire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "Missing entity: iPhone X release date\n"
     ]
    }
   ],
   "source": [
    "text = \"Upcoming iPhone X release date leaked as Apple reveals pre-orders\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and label\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# Get the span for \"iPhone X\"\n",
    "iphone_x  = doc[1:5]\n",
    "\n",
    "# Print the span text\n",
    "print(\"Missing entity:\", iphone_x.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "completed-agency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "PRP\n",
      "0.0\n",
      "had\n",
      "VBD\n",
      "0.0\n",
      "a\n",
      "DT\n",
      "0.0\n",
      "challenge\n",
      "NN\n",
      "0.0\n",
      "accessing\n",
      "VBG\n",
      "0.0\n",
      "this\n",
      "DT\n",
      "0.0\n",
      "form\n",
      "NN\n",
      "0.0\n",
      ".\n",
      ".\n",
      "0.0\n",
      "Is\n",
      "VBZ\n",
      "0.0\n",
      "it\n",
      "PRP\n",
      "0.0\n",
      "possible\n",
      "JJ\n",
      "0.0\n",
      "to\n",
      "TO\n",
      "0.0\n",
      "allocate\n",
      "VB\n",
      "0.0\n",
      "a\n",
      "DT\n",
      "0.0\n",
      "channel\n",
      "NN\n",
      "0.0\n",
      "for\n",
      "IN\n",
      "0.0\n",
      "this\n",
      "DT\n",
      "0.0\n",
      "form\n",
      "NN\n",
      "0.0\n",
      "?\n",
      ".\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "text = \"I had a challenge accessing this form. Is it possible to allocate a channel for this form?\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "#for ent in doc.ents:\n",
    "#    print(ent.text, ent.label_, ent.sentiment)\n",
    "    \n",
    "for token in doc:\n",
    "    print(token.text,  )\n",
    "    print( token.tag_,)\n",
    "    #print(token.similarity,)\n",
    "    print(token.sentiment)\n",
    "    \n",
    "#token.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-blowing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
